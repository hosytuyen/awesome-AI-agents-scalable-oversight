# üß† Awesome Papers on Scalable Oversight

A curated collection of research papers on **Scalable Oversight**

Automatically updated database from arXiv to monitor the latest developments in the field.

*If you want to create a similar automated curated collection on your own topics, check out our simple tool in the `paper-agent` folder! If you find this useful, give me a star ‚≠ê Thank you!!!*

<div style="overflow-x: auto; white-space: nowrap;">

<small>

| # | üß† Title | üè∑Ô∏è Tags | üìÖ Published Date | üîó arXiv URL | üí° Key Insights |
|---|-----------|--------|------------------|--------------|----------------|
| 1 | [AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs](http://arxiv.org/abs/2510.13912v2) | `AI Debate`, `scalable oversight`, `Language Model Alignment`, `Persuasion`, `Belief Elicitation`, `Sycophancy` | 2025-10-15 | [Link](http://arxiv.org/abs/2510.13912v2) | AI debaters are more persuasive when arguing in alignment with their own beliefs, suggesting that truthful arguments (aligned with internal models) are inherently more convincing. This has implication... |
| 2 | [OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment](http://arxiv.org/abs/2510.07743v1) | `reward modeling`, `rubrics`, `RLHF`, `LLM Alignment`, `scalable oversight` | 2025-10-09 | [Link](http://arxiv.org/abs/2510.07743v1) | Rubrics offer a structured and multifaceted approach to reward modeling, potentially capturing more nuanced human preferences than scalar or pairwise judgments, leading to better alignment., The Contr... |
| 3 | [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](http://arxiv.org/abs/2509.13334v1) | `Chain-of-Thought Reasoning`, `Faithfulness`, `Causal Intervention`, `Direct Preference Optimization`, `scalable alignment` | 2025-09-10 | [Link](http://arxiv.org/abs/2509.13334v1) | The paper addresses a critical weakness in Chain-of-Thought (CoT) reasoning: the lack of causal influence of reasoning steps on the final answer, which leads to untrustworthy outputs. This is directly... |
| 4 | [Scaling behavior of large language models in emotional safety classification across sizes and tasks](http://arxiv.org/abs/2509.04512v1) | `Emotional Safety`, `LLM Safety`, `scalable alignment`, `Mental Health Applications`, `Fine-tuning` | 2025-09-02 | [Link](http://arxiv.org/abs/2509.04512v1) | Larger LLMs generally perform better in emotional safety classification, especially in zero-shot and nuanced multi-label scenarios, indicating a scaling effect for safety-related tasks., Fine-tuning s... |
| 5 | [WST: Weak-to-Strong Knowledge Transfer via Reinforcement Learning](http://arxiv.org/abs/2508.16741v1) | `prompt engineering`, `Weak-to-Strong Transfer`, `reinforcement learning`, `LLM Alignment`, `scalable oversight` | 2025-08-22 | [Link](http://arxiv.org/abs/2508.16741v1) | Weak-to-strong transfer (WST) offers a scalable approach to improving LLM performance and alignment by using a small, easily auditable 'Teacher' model to generate instructions for a larger 'Student' m... |
| 6 | [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](http://arxiv.org/abs/2508.03858v2) | `Runtime Governance`, `Agentic AI`, `AI safety`, `scalable oversight`, `risk management` | 2025-08-05 | [Link](http://arxiv.org/abs/2508.03858v2) | Runtime governance is crucial for agentic AI systems due to their emergent behaviors, which cannot be fully addressed by pre-deployment strategies alone., The MI9 framework offers a comprehensive appr... |
| 7 | [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](http://arxiv.org/abs/2507.19672v1) | `LLM Alignment`, `scalable oversight`, `Safety Mechanisms`, `preference learning`, `robustness` | 2025-07-25 | [Link](http://arxiv.org/abs/2507.19672v1) | The paper highlights the limitations of current evaluation frameworks for LLM alignment, specifically mentioning reward misspecification, distributional robustness, and *scalable oversight* as key cha... |
| 8 | [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](http://arxiv.org/abs/2507.11473v1) | `Chain of Thought`, `Monitorability`, `AI safety`, `scalable oversight`, `Intent Detection` | 2025-07-15 | [Link](http://arxiv.org/abs/2507.11473v1) | Chain of Thought (CoT) reasoning in language models presents a novel opportunity for AI safety by allowing monitoring of the model's internal reasoning process for signs of malicious intent., CoT moni... |
| 9 | [VerificAgent: Domain-Specific Memory Verification for Scalable Oversight of Aligned Computer-Use Agents](http://arxiv.org/abs/2506.02539v3) | `scalable oversight`, `memory verification`, `computer-using agents`, `Alignment`, `human-in-the-loop`, `policy drift`, `domain-specific` | 2025-06-03 | [Link](http://arxiv.org/abs/2506.02539v3) | Verifying and sanitizing the memory of computer-using agents (CUAs) provides a scalable oversight mechanism by preventing the accumulation of unsafe or domain-inappropriate heuristics., Human fact-che... |
| 10 | [AI Debate Aids Assessment of Controversial Claims](http://arxiv.org/abs/2506.02175v1) | `AI Debate`, `scalable oversight`, `Bias Mitigation`, `Truth Seeking`, `COVID-19 Factuality` | 2025-06-02 | [Link](http://arxiv.org/abs/2506.02175v1) | AI debate can effectively improve human judgment accuracy and confidence calibration on controversial topics, even when judges hold strong prior beliefs., AI judges with human-like personas can achiev... |
| 11 | [Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks](http://arxiv.org/abs/2505.23820v1) | `LLMs`, `Alignment`, `scalable oversight`, `No-Consensus Tasks`, `Bias` | 2025-05-28 | [Link](http://arxiv.org/abs/2505.23820v1) | LLMs exhibit a tendency to take a stance on no-consensus topics when acting as judges or debaters, highlighting a limitation in replicating human disagreement., The study reveals that LLMs, while capa... |
| 12 | [Preference Learning with Lie Detectors can Induce Honesty or Evasion](http://arxiv.org/abs/2505.13787v1) | `scalable oversight`, `Deception`, `preference learning`, `Lie Detection`, `AI Alignment` | 2025-05-20 | [Link](http://arxiv.org/abs/2505.13787v1) | Incorporating lie detectors into the preference learning pipeline of LLMs can have unintended consequences, potentially leading to policies that evade detection while remaining deceptive, highlighting... |
| 13 | [Debating for Better Reasoning: An Unsupervised Multimodal Approach](http://arxiv.org/abs/2505.14627v1) | `debate`, `scalable oversight`, `multimodal learning`, `visual question answering`, `AI Alignment` | 2025-05-20 | [Link](http://arxiv.org/abs/2505.14627v1) | The debate framework, even with a weaker, text-only judge, can effectively supervise and improve the performance of stronger, multimodal models, suggesting a pathway for scalable oversight where human... |
| 14 | [Confirmation bias: A challenge for scalable oversight](http://arxiv.org/abs/2507.19486v1) | `scalable oversight`, `confirmation bias`, `Human-AI Interaction`, `AI safety`, `Cognitive Bias` | 2025-05-17 | [Link](http://arxiv.org/abs/2507.19486v1) | Confirmation bias significantly hinders the effectiveness of simple scalable oversight protocols, even when evaluators are aware of the model's potential for errors., Online research, intended to impr... |
| 15 | [Rethinking LLM Advancement: Compute-Dependent and Independent Paths to Progress](http://arxiv.org/abs/2505.04075v2) | `Algorithmic Innovation`, `compute-independent advancements`, `compute-dependent advancements`, `AI Oversight`, `capability forecasting` | 2025-05-07 | [Link](http://arxiv.org/abs/2505.04075v2) | Restricting compute alone is insufficient for controlling LLM capabilities, as algorithmic innovations can lead to significant performance gains even in compute-constrained environments., AI oversight... |
| 16 | [DeepCritic: Deliberate Critique with Large Language Models](http://arxiv.org/abs/2505.00662v1) | `LLM Critique`, `Automated Oversight`, `reinforcement learning`, `Math Reasoning`, `Error Identification` | 2025-05-01 | [Link](http://arxiv.org/abs/2505.00662v1) | LLMs can be effectively trained to provide detailed, step-wise critiques of other LLMs' reasoning processes, significantly improving error identification accuracy compared to existing methods., A two-... |
| 17 | [Scaling Laws For Scalable Oversight](http://arxiv.org/abs/2504.18530v2) | `scalable oversight`, `AI safety`, `nested oversight`, `capability mismatch`, `scaling laws` | 2025-04-25 | [Link](http://arxiv.org/abs/2504.18530v2) | The paper introduces a framework for quantifying the probability of successful oversight based on the capabilities of the overseer and the overseen system, modeled as a game with oversight-specific El... |
| 18 | [Super Co-alignment of Human and AI for Sustainable Symbiotic Society](http://arxiv.org/abs/2504.17404v5) | `superalignment`, `co-alignment`, `scalable oversight`, `AI safety`, `value alignment`, `symbiotic AI` | 2025-04-24 | [Link](http://arxiv.org/abs/2504.17404v5) | The paper argues that unidirectional imposition of human values on superintelligent AI is insufficient for true alignment, advocating for a 'Super Co-alignment' approach where values are co-shaped by ... |
| 19 | [Adversarial Training of Reward Models](http://arxiv.org/abs/2504.06141v2) | `reward modeling`, `Adversarial Training`, `robustness`, `reward hacking`, `RLHF`, `scalable oversight` | 2025-04-08 | [Link](http://arxiv.org/abs/2504.06141v2) | Adversarial training of reward models is crucial for preventing reward hacking and improving the robustness of RLHF systems, directly addressing a key challenge in scalable oversight., The Adv-RM fram... |
| 20 | [A Benchmark for Scalable Oversight Protocols](http://arxiv.org/abs/2504.03731v1) | `scalable oversight`, `AI Alignment`, `human feedback`, `Benchmarking`, `debate` | 2025-03-31 | [Link](http://arxiv.org/abs/2504.03731v1) | The paper addresses a critical gap in the field by providing a systematic empirical framework for evaluating scalable oversight protocols, which is essential for ensuring AI alignment as agents become... |
| 21 | [FindTheFlaws: Annotated Errors for Detecting Flawed Reasoning and Scalable Oversight Research](http://arxiv.org/abs/2503.22989v1) | `scalable oversight`, `AI safety`, `error detection`, `critique`, `dataset` | 2025-03-29 | [Link](http://arxiv.org/abs/2503.22989v1) | The FindTheFlaws dataset directly addresses the need for high-quality, annotated data to evaluate and improve scalable oversight techniques like debate, critique, and prover-verifier games., The perfo... |
| 22 | [Superalignment with Dynamic Human Values](http://arxiv.org/abs/2503.13621v1) | `scalable oversight`, `Dynamic Human Values`, `Recursive Reward Modeling`, `Task Decomposition`, `Part-to-Complete Generalization` | 2025-03-17 | [Link](http://arxiv.org/abs/2503.13621v1) | The paper directly addresses the challenge of scalable oversight by proposing a task decomposition approach, aiming to break down complex tasks into human-understandable and supervisable subtasks., It... |
| 23 | [Modeling Human Beliefs about AI Behavior for Scalable Oversight](http://arxiv.org/abs/2502.21262v2) | `scalable oversight`, `belief modeling`, `value learning`, `human feedback`, `AI Alignment` | 2025-02-28 | [Link](http://arxiv.org/abs/2502.21262v2) | Modeling human beliefs about AI behavior is crucial for reliable value learning, especially when AI systems surpass human capabilities and evaluators may misunderstand the AI's actions., The paper int... |
| 24 | [Scalable Oversight for Superhuman AI via Recursive Self-Critiquing](http://arxiv.org/abs/2502.04675v3) | `scalable oversight`, `recursive self-critiquing`, `AI Alignment`, `Human-AI Interaction`, `AI-AI interaction` | 2025-02-07 | [Link](http://arxiv.org/abs/2502.04675v3) | Recursive self-critiquing offers a potential pathway to scalable oversight by shifting the burden from direct human assessment of complex AI outputs to evaluating critiques of critiques, which may be ... |
| 25 | [Great Models Think Alike and this Undermines AI Oversight](http://arxiv.org/abs/2502.04313v2) | `AI Oversight`, `Model Similarity`, `Weak-to-Strong Generalization`, `Correlated Failures`, `Language Model Evaluation` | 2025-02-06 | [Link](http://arxiv.org/abs/2502.04313v2) | LLM-as-a-judge systems exhibit self-preference, favoring models similar to themselves, which can bias evaluation and oversight., Gains from weak-to-strong generalization in AI oversight are heavily in... |
| 26 | [The Right to AI](http://arxiv.org/abs/2501.17899v2) | `Participatory AI`, `AI Governance`, `scalable oversight`, `Human-Centered AI`, `AI Alignment` | 2025-01-29 | [Link](http://arxiv.org/abs/2501.17899v2) | The paper highlights the importance of community involvement in AI development and governance, which is crucial for scalable oversight. By incorporating diverse perspectives, we can potentially mitiga... |
| 27 | [Debate Helps Weak-to-Strong Generalization](http://arxiv.org/abs/2501.13124v1) | `Weak-to-Strong Generalization`, `scalable oversight`, `debate`, `AI Alignment`, `Human Supervision` | 2025-01-21 | [Link](http://arxiv.org/abs/2501.13124v1) | Debate can be a valuable mechanism for a weak model to extract reliable information from a stronger, potentially untrustworthy model, improving the weak model's performance., Ensembling weak models tr... |
| 28 | [Understanding Impact of Human Feedback via Influence Functions](http://arxiv.org/abs/2501.05790v3) | `RLHF`, `Influence Functions`, `reward modeling`, `human feedback`, `Bias Detection`, `scalable oversight` | 2025-01-10 | [Link](http://arxiv.org/abs/2501.05790v3) | Influence functions offer a promising method for understanding and mitigating the impact of noisy, inconsistent, or biased human feedback in RLHF, which is crucial for aligning LLMs with human intenti... |
| 29 | [The Road to Artificial SuperIntelligence: A Comprehensive Survey of Superalignment](http://arxiv.org/abs/2412.16468v3) | `superalignment`, `scalable oversight`, `ASI`, `AI safety`, `alignment paradigms` | 2024-12-21 | [Link](http://arxiv.org/abs/2412.16468v3) | The paper highlights the limitations of current alignment techniques when applied to Artificial Superintelligence (ASI), emphasizing the need for novel approaches that can scale to superhuman capabili... |
| 30 | [The Superalignment of Superhuman Intelligence with Large Language Models](http://arxiv.org/abs/2412.11145v2) | `superalignment`, `scalable oversight`, `Weak-to-Strong Generalization`, `Adversarial Training`, `AI safety` | 2024-12-15 | [Link](http://arxiv.org/abs/2412.11145v2) | The paper explicitly addresses the problem of aligning superhuman AI, particularly LLMs, with human values, which is a core concern in scalable oversight research., It highlights the challenges of sca... |
| 31 | [Rethinking LLM Uncertainty: A Multi-Agent Approach to Estimating Black-Box Model Uncertainty](http://arxiv.org/abs/2412.09572v2) | `uncertainty estimation`, `LLM`, `scalable oversight`, `hallucination detection`, `multi-agent systems` | 2024-12-12 | [Link](http://arxiv.org/abs/2412.09572v2) | Existing self-consistency methods for uncertainty estimation in LLMs can be misleading due to contextual biases affecting knowledge retrieval., A multi-agent approach, leveraging diverse query variati... |
| 32 | [ProcessBench: Identifying Process Errors in Mathematical Reasoning](http://arxiv.org/abs/2412.06559v4) | `scalable oversight`, `error detection`, `mathematical reasoning`, `process reward models`, `critic models` | 2024-12-09 | [Link](http://arxiv.org/abs/2412.06559v4) | The paper highlights the limitations of existing process reward models (PRMs) in generalizing to more complex mathematical reasoning problems, indicating a need for more robust and generalizable overs... |
| 33 | [Balancing Label Quantity and Quality for Scalable Elicitation](http://arxiv.org/abs/2410.13215v2) | `scalable oversight`, `label quality`, `label quantity`, `pretrained models`, `sample efficiency`, `elicitation`, `cost-benefit analysis` | 2024-10-17 | [Link](http://arxiv.org/abs/2410.13215v2) | There exists a quantity-quality tradeoff in labeling data for training AI models, particularly when using pretrained models. Understanding this tradeoff is crucial for scalable oversight, as it impact... |
| 34 | [Gradient Routing: Masking Gradients to Localize Computation in Neural Networks](http://arxiv.org/abs/2410.04332v2) | `scalable oversight`, `Modular Neural Networks`, `Interpretability`, `Unlearning`, `Reinforcement Learning Alignment` | 2024-10-06 | [Link](http://arxiv.org/abs/2410.04332v2) | Gradient routing offers a method to enforce modularity in neural networks, allowing for more targeted interventions and oversight. This is crucial for scalable oversight as it allows focusing on speci... |
| 35 | [Possible Principles for Aligned Structure Learning Agents](http://arxiv.org/abs/2410.00258v3) | `scalable alignment`, `Structure Learning`, `Causal Representation Learning`, `Theory of Mind`, `preference learning` | 2024-09-30 | [Link](http://arxiv.org/abs/2410.00258v3) | The paper highlights the importance of structure learning (causal representation learning) as a foundation for scalable alignment. Agents that can accurately model the world, including other agents' p... |
| 36 | [Training Language Models to Win Debates with Self-Play Improves Judge Accuracy](http://arxiv.org/abs/2409.16636v1) | `debate`, `scalable oversight`, `Language Models`, `self-play`, `judge accuracy` | 2024-09-25 | [Link](http://arxiv.org/abs/2409.16636v1) | Training language models to win debates through self-play can improve the accuracy of language model-based judges in evaluating complex tasks, suggesting a potential path for scalable oversight., Deba... |
| 37 | [Explanation, Debate, Align: A Weak-to-Strong Framework for Language Model Generalization](http://arxiv.org/abs/2409.07335v1) | `Weak-to-Strong Generalization`, `model alignment`, `Explanation Generation`, `multi-agent systems`, `scalable oversight` | 2024-09-11 | [Link](http://arxiv.org/abs/2409.07335v1) | Weak-to-strong generalization offers a pathway for transferring alignment properties from advanced, potentially more aligned, models to weaker ones, reducing the need for extensive, potentially biased... |
| 38 | [AI-Assisted Generation of Difficult Math Questions](http://arxiv.org/abs/2407.21009v4) | `LLM`, `Question Generation`, `Difficulty`, `Metacognition`, `human-in-the-loop`, `Out-of-Distribution`, `scalable oversight` | 2024-07-30 | [Link](http://arxiv.org/abs/2407.21009v4) | The paper demonstrates a method for generating 'out-of-distribution' (OOD) examples that are difficult for both LLMs and humans. This is valuable for scalable oversight because OOD examples can expose... |
| 39 | [ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models](http://arxiv.org/abs/2407.04693v2) | `hallucination detection`, `scalable oversight`, `self-training`, `expectation maximization`, `LLM Evaluation` | 2024-07-05 | [Link](http://arxiv.org/abs/2407.04693v2) | The paper introduces a scalable approach to detect and mitigate hallucinations in LLMs, a critical aspect of ensuring AI safety and reliability. By automating the annotation process, it addresses the ... |
| 40 | [On scalable oversight with weak LLMs judging strong LLMs](http://arxiv.org/abs/2407.04622v2) | `scalable oversight`, `debate`, `Language Models`, `AI Alignment`, `Weak-to-Strong Generalization` | 2024-07-05 | [Link](http://arxiv.org/abs/2407.04622v2) | Debate, where two AI agents argue to convince a weaker AI judge, consistently outperforms consultancy (single AI trying to convince a judge) when the consultant is randomly assigned to argue for a spe... |
| 41 | [Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision](http://arxiv.org/abs/2403.09472v2) | `scalable oversight`, `reward modeling`, `Easy-to-Hard Generalization`, `AI Alignment`, `reasoning` | 2024-03-14 | [Link](http://arxiv.org/abs/2403.09472v2) | Training reward models on easier tasks can enable effective evaluation and improvement of AI agents on harder tasks, even surpassing human-level performance on those harder tasks., Process supervision... |
| 42 | [On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models](http://arxiv.org/abs/2403.04204v1) | `Alignment`, `big models`, `scalable oversight`, `RLHF`, `SFT` | 2024-03-07 | [Link](http://arxiv.org/abs/2403.04204v1) | The paper highlights the inherent challenges in aligning large models, including data costs and scalable oversight, emphasizing that finding the optimal alignment strategy remains an open problem., Th... |
| 43 | [CriticEval: Evaluating Large Language Model as Critic](http://arxiv.org/abs/2402.13764v5) | `LLM Evaluation`, `critique ability`, `scalable oversight`, `AI safety`, `benchmark` | 2024-02-21 | [Link](http://arxiv.org/abs/2402.13764v5) | The paper introduces a benchmark (CriticEval) specifically designed to evaluate the critique ability of LLMs, which is a critical component for scalable oversight as it allows LLMs to identify and cor... |
| 44 | [Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning](http://arxiv.org/abs/2402.00667v1) | `Weak-to-Strong Generalization`, `scalable oversight`, `Ensemble Learning`, `Human-AI Interaction`, `AI Alignment` | 2024-02-01 | [Link](http://arxiv.org/abs/2402.00667v1) | The paper directly addresses the challenge of aligning increasingly capable AI systems with human values through the Weak-to-Strong Generalization (W2SG) framework, which is a core concern in scalable... |
| 45 | [Reinforcement Learning from LLM Feedback to Counteract Goal Misgeneralization](http://arxiv.org/abs/2401.07181v1) | `goal misgeneralization`, `LLM feedback`, `reinforcement learning`, `scalable oversight`, `reward shaping` | 2024-01-14 | [Link](http://arxiv.org/abs/2401.07181v1) | LLMs can provide effective, scalable oversight for RL agents, even without task proficiency, by identifying potential goal misgeneralization scenarios., Using LLM feedback to shape the reward function... |
| 46 | [The Unreasonable Effectiveness of Easy Training Data for Hard Tasks](http://arxiv.org/abs/2401.06751v2) | `scalable oversight`, `Easy-to-Hard Generalization`, `Language Models`, `Data Efficiency`, `Weak Supervision` | 2024-01-12 | [Link](http://arxiv.org/abs/2401.06751v2) | Language models exhibit surprisingly strong generalization from easy to hard data, potentially reducing the need for expensive and noisy hard-labeled data in some contexts., Collecting and utilizing e... |
| 47 | [GPQA: A Graduate-Level Google-Proof Q&A Benchmark](http://arxiv.org/abs/2311.12022v1) | `scalable oversight`, `AI Alignment`, `benchmark`, `Question Answering`, `expert knowledge` | 2023-11-20 | [Link](http://arxiv.org/abs/2311.12022v1) | GPQA provides a challenging benchmark for evaluating scalable oversight methods, as it requires AI systems to answer questions that are difficult even for skilled humans with access to external inform... |
| 48 | [A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning](http://arxiv.org/abs/2311.07954v2) | `self-verification`, `logical reasoning`, `fallacy detection`, `LLMs`, `scalable oversight` | 2023-11-14 | [Link](http://arxiv.org/abs/2311.07954v2) | LLMs struggle to accurately identify logical fallacies in their own reasoning, suggesting limitations in their self-verification abilities., The inability to reliably detect fallacies undermines the e... |
| 49 | [SALMON: Self-Alignment with Instructable Reward Models](http://arxiv.org/abs/2310.05910v2) | `reward modeling`, `scalable alignment`, `Principle-Based Alignment`, `LLM Alignment`, `Synthetic Data` | 2023-10-09 | [Link](http://arxiv.org/abs/2310.05910v2) | SALMON offers a pathway to reduce reliance on extensive human feedback by using an instructable reward model trained on synthetic preference data derived from human-defined principles. This is crucial... |
| 50 | [Assessing Large Language Models on Climate Information](http://arxiv.org/abs/2310.02932v2) | `scalable oversight`, `LLM Evaluation`, `Climate Change Communication`, `Epistemological Adequacy`, `AI Assistance` | 2023-10-04 | [Link](http://arxiv.org/abs/2310.02932v2) | LLMs exhibit a significant gap between surface-level fluency and epistemological accuracy when communicating about climate change, highlighting the need for careful oversight to ensure factual correct... |
| 51 | [Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK](http://arxiv.org/abs/2304.11218v1) | `explainability`, `AI policy`, `regulation`, `AI Oversight`, `transparency` | 2023-04-20 | [Link](http://arxiv.org/abs/2304.11218v1) | Current AI policies regarding explainability are often based on coarse notions and requirements, potentially hindering practical adoption and effective oversight., A lack of consensus on what constitu... |
| 52 | [Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks](http://arxiv.org/abs/2211.10024v3) | `scalable oversight`, `Adversarial Attacks`, `Interpretability`, `Red Teaming`, `automated diagnostics` | 2022-11-18 | [Link](http://arxiv.org/abs/2211.10024v3) | Automated methods like SNAFUE can significantly enhance scalable oversight by efficiently identifying vulnerabilities in DNNs that might be missed by human reviewers or traditional adversarial example... |
| 53 | [Measuring Progress on Scalable Oversight for Large Language Models](http://arxiv.org/abs/2211.03540v2) | `scalable oversight`, `Language Models`, `Human-AI Collaboration`, `Alignment`, `empirical evaluation` | 2022-11-04 | [Link](http://arxiv.org/abs/2211.03540v2) | Human-AI collaboration, even with an unreliable AI assistant, can significantly outperform both unaided humans and the AI model alone on complex tasks., The paper provides a concrete experimental desi... |

</small>
</div>